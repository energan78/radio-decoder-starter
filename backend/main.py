import os
import json
import re
import logging
from fastapi import FastAPI, UploadFile, File, HTTPException, Request, Form, Body
from fastapi.responses import JSONResponse, HTMLResponse, FileResponse
from pydantic import BaseModel, HttpUrl
import httpx
import aiofiles
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from backend.train_signal_model import ConvSignalNet, SAMPLE_LEN
import speech_recognition as sr
from fastapi import UploadFile, File
from pydub import AudioSegment
from vosk import Model, KaldiRecognizer
import wave
import json
import subprocess
from backend.signal_utils import (
    load_signal, extract_freq_from_filename, get_class_stats, create_class_folder
)
from backend.match_freq_band import match_frequency
from fastapi import Body
from backend.analyzer import analyze_signal_file
import joblib
from backend.freq_db import FREQ_DB

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Radio Decoder API",
    description="API –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–¥–∏–æ—Ñ–∞–π–ª–æ–≤",
    version="1.0.0"
)

class FetchRequest(BaseModel):
    url: HttpUrl

def load_config():
    try:
        with open("backend/config.json") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ config.json: {e}")
        # –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        return {
            "use_anomaly_detector": False,
            "use_geo_classifier": False
        }

config = load_config()

UPLOAD_FOLDER = "backend/uploads"
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

SIGNAL_LIBRARY_PATH = "backend/signal_library"
os.makedirs(SIGNAL_LIBRARY_PATH, exist_ok=True)

def extract_frequency(filename):
    match = re.search(r"(\d{3,4})MHz", filename)
    if match:
        return match.group(1)
    return "unknown"

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"–û—à–∏–±–∫–∞: {exc}")
    return JSONResponse(
        status_code=500,
        content={"detail": "–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞"}
    )

@app.post("/upload", summary="–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª", description="–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–¥–∏–æ—Ñ–∞–π–ª–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞.")
async def upload_file(file: UploadFile = File(...)):
    file_location = os.path.join(UPLOAD_FOLDER, file.filename)
    try:
        async with aiofiles.open(file_location, "wb") as f:
            content = await file.read()
            await f.write(content)
        freq = extract_frequency(file.filename)
        logger.info(f"–§–∞–π–ª {file.filename} –∑–∞–≥—Ä—É–∂–µ–Ω. –ß–∞—Å—Ç–æ—Ç–∞: {freq}")
        return {"filename": file.filename, "frequency": freq}
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞")

@app.post("/fetch", summary="–ü–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ URL", description="–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–º—É URL.")
async def fetch_url(request: FetchRequest):
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(request.url)
        response.raise_for_status()
        logger.info(f"–£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ URL: {request.url}")
        return {"content": response.text}
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ URL: {e}")
        raise HTTPException(status_code=400, detail=str(e))

# –ü—Ä–∏–º–µ—Ä –ø—Ä–æ—Å—Ç–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–æ–≤
class SimpleSignalNet(nn.Module):
    def __init__(self, num_classes):
        super(SimpleSignalNet, self).__init__()
        self.fc1 = nn.Linear(1024, 128)
        self.fc2 = nn.Linear(128, num_classes)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# –ó–∞–≥–ª—É—à–∫–∞: —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤ (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä—è—Ç—å)
def get_signal_classes():
    classes = []
    for d in os.listdir(SIGNAL_LIBRARY_PATH):
        if os.path.isdir(os.path.join(SIGNAL_LIBRARY_PATH, d)):
            classes.append(d)
    return classes if classes else ["FM", "AM", "GSM", "WiFi"]

MODEL_PATH = "backend/signal_model.pth"

def load_model():
    checkpoint = torch.load(MODEL_PATH, map_location="cpu")
    classes = checkpoint["classes"]
    model = ConvSignalNet(len(classes))
    model.load_state_dict(checkpoint["model_state"])
    model.eval()
    return model, classes

# –ü—Ä–∏–º–µ—Ä —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–∞
def classify_signal_with_model(iq_data: np.ndarray):
    model, classes = load_model()
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º IQ-–¥–∞–Ω–Ω—ã–µ –∫ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1024)
    if len(iq_data) < 1024:
        iq_data = np.pad(iq_data, (0, 1024 - len(iq_data)), 'constant')
    else:
        iq_data = iq_data[:1024]
    x = np.abs(iq_data).astype(np.float32)
    x = torch.tensor(x).unsqueeze(0)
    with torch.no_grad():
        logits = model(x)
        probs = F.softmax(logits, dim=1).numpy()[0]
        idx = np.argmax(probs)
        return classes[idx], float(probs[idx])

class SignalTypeResponse(BaseModel):
    signal_type: str
    confidence: float

@app.post("/classify_signal", response_model=SignalTypeResponse, summary="–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ —Ä–∞–¥–∏–æ—Å–∏–≥–Ω–∞–ª–∞")
async def classify_signal(file: UploadFile = File(...)):
    try:
        content = await file.read()
        iq_data = np.frombuffer(content, dtype=np.complex64)
        signal_type, confidence = classify_signal_with_model(iq_data)
        logger.info(f"–§–∞–π–ª {file.filename} –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω –∫–∞–∫ {signal_type} ({confidence:.2f})")
        return SignalTypeResponse(signal_type=signal_type, confidence=confidence)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–∞: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–∏–≥–Ω–∞–ª–∞")

# –≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫—É
class AddSignalRequest(BaseModel):
    signal_type: str

@app.post("/add_signal", summary="–î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä —Å–∏–≥–Ω–∞–ª–∞ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫—É")
async def add_signal(
    file: UploadFile = File(...),
    signal_type: str = Form(...),
    comment: str = Form(None)
):
    save_dir = f"backend/signal_library/{signal_type}"
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, file.filename)
    with open(save_path, "wb") as f_out:
        f_out.write(await file.read())
    if comment:
        with open(save_path + ".txt", "w", encoding="utf-8") as f_comment:
            f_comment.write(comment)
    from backend.signal_utils import load_signal
    data = load_signal(save_path)
    info = {
        "length": len(data),
        "dtype": str(data.dtype)
    }
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á–∞—Å—Ç–æ—Ç—É –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
    freq_mhz = extract_freq_from_filename(file.filename)
    if freq_mhz is not None:
        # –¥–∞–ª–µ–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ freq_mhz –¥–ª—è match_frequency –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
        comment_data = match_frequency(freq_mhz)
        comment = f"{comment_data['label']} ‚Äî {comment_data['usage']} (–º–æ–¥—É–ª—è—Ü–∏—è: {comment_data['modulation']})"

    return {"status": "ok", "info": info}

VOSK_MODEL_PATH = "backend/vosk-model-ru"

class SpeechRecognitionResponse(BaseModel):
    text: str
    comment: str

@app.post("/recognize_speech", response_model=SpeechRecognitionResponse, summary="–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∏–∑ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞")
async def recognize_speech(file: UploadFile = File(...)):
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π WAV
        temp_path = f"/tmp/{file.filename}"
        async with aiofiles.open(temp_path, "wb") as f:
            await f.write(await file.read())

        # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ WAV, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ WAV
        if not temp_path.lower().endswith(".wav"):
            audio = AudioSegment.from_file(temp_path)
            temp_wav = temp_path + ".wav"
            audio.export(temp_wav, format="wav")
            temp_path = temp_wav

        recognizer = sr.Recognizer()
        with sr.AudioFile(temp_path) as source:
            audio = recognizer.record(source)
        try:
            text = recognizer.recognize_google(audio, language="ru-RU")
            comment = "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Ä–µ—á—å" if text.strip() else "–†–µ—á—å –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞"
        except sr.UnknownValueError:
            text = ""
            comment = "–†–µ—á—å –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞"
        except Exception as e:
            text = ""
            comment = f"–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {e}"

        logger.info(f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {text}")
        return SpeechRecognitionResponse(text=text, comment=comment)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏")

@app.post("/recognize_speech_offline", response_model=SpeechRecognitionResponse, summary="–û—Ñ–ª–∞–π–Ω-—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∏–∑ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞")
async def recognize_speech_offline(file: UploadFile = File(...)):
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª –≤–æ –≤—Ä–µ–º–Ω–∏–π WAV
        temp_path = f"/tmp/{file.filename}"
        async with aiofiles.open(temp_path, "wb") as f:
            await f.write(await file.read())

        # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ WAV, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ WAV
        if not temp_path.lower().endswith(".wav"):
            audio = AudioSegment.from_file(temp_path)
            temp_wav = temp_path + ".wav"
            audio.export(temp_wav, format="wav")
            temp_path = temp_wav

        # –û—Ç–∫—Ä—ã–≤–∞–µ–º WAV-—Ñ–∞–π–ª
        wf = wave.open(temp_path, "rb")
        if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() not in [8000, 16000, 44100]:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            audio = AudioSegment.from_file(temp_path)
            audio = audio.set_channels(1).set_frame_rate(16000).set_sample_width(2)
            temp_fixed = temp_path + ".fixed.wav"
            audio.export(temp_fixed, format="wav")
            wf = wave.open(temp_fixed, "rb")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å Vosk
        if not os.path.exists(VOSK_MODEL_PATH):
            raise Exception("Vosk model not found!")
        model = Model(VOSK_MODEL_PATH)
        rec = KaldiRecognizer(model, wf.getframerate())
        rec.SetWords(True)

        # –†–∞—Å–ø–æ–∑–Ω–∞—ë–º –∞—É–¥–∏–æ
        result_text = ""
        while True:
            data = wf.readframes(4000)
            if len(data) == 0:
                break
            if rec.AcceptWaveform(data):
                part = json.loads(rec.Result())
                result_text += part.get("text", "") + " "
        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        part = json.loads(rec.FinalResult())
        result_text += part.get("text", "")

        result_text = result_text.strip()
        comment = "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Ä–µ—á—å" if result_text else "–†–µ—á—å –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞"

        logger.info(f"Vosk –æ—Ñ–ª–∞–π–Ω-—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ: {result_text}")
        return SpeechRecognitionResponse(text=result_text, comment=comment)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ—Ñ–ª–∞–π–Ω-—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –æ—Ñ–ª–∞–π–Ω-—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏")

@app.get("/signal_classes")
def signal_classes():
    stats = get_class_stats()
    return {"classes": list(stats.keys())}

@app.post("/create_class_folder")
def create_class_folder_api(class_name: str = Body(..., embed=True)):
    create_class_folder(class_name)
    return {"status": "ok"}

@app.post("/analyze_signal")
async def analyze_signal(
    file: UploadFile = File(...),
    signal_type: str = Form(...),
    comment: str = Form(None)
):
    temp_path = f"backend/temp/{file.filename}"
    os.makedirs(os.path.dirname(temp_path), exist_ok=True)
    with open(temp_path, "wb") as f_out:
        f_out.write(await file.read())

    freq_mhz = extract_freq_from_filename(file.filename)
    band_info = match_frequency(freq_mhz) if freq_mhz else None
    comment_auto = None
    if band_info:
        comment_auto = f"{band_info['label']} ‚Äî {band_info['usage']} (–º–æ–¥—É–ª—è—Ü–∏—è: {band_info['modulation']})"

    result = analyze_signal_file(
        temp_path,
        model_pytorch=model_pytorch,
        model_rf=model_rf,
        model_svm=model_svm,
        sample_len=sample_len
    )

    return {
        **result,
        "freq_mhz": freq_mhz,
        "label": band_info["label"] if band_info else None,
        "modulation": band_info["modulation"] if band_info else None,
        "usage": band_info["usage"] if band_info else None,
        "comment_auto": comment_auto,
    }

@app.get("/settings")
def get_settings():
    return config

@app.post("/settings")
def update_settings(new_settings: dict):
    config.update(new_settings)
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ñ–∞–π–ª
    with open("backend/config.json", "w") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    return config

@app.get("/settings_ui", response_class=HTMLResponse)
def settings_ui():
    return FileResponse("backend/settings.html")

@app.get("/logs_ui", response_class=HTMLResponse)
def logs_ui():
    return FileResponse("backend/logs_ui.html")

@app.get("/upload_ui", response_class=HTMLResponse)
def upload_ui():
    return FileResponse("backend/upload_ui.html")

@app.get("/logs")
def get_logs():
    log_path = "backend/app.log"
    if not os.path.exists(log_path):
        return "–õ–æ–≥-—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω."
    with open(log_path, "r", encoding="utf-8") as f:
        return f.read()[-10000:]  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10–∫ —Å–∏–º–≤–æ–ª–æ–≤

@app.get("/status_ui", response_class=HTMLResponse)
def status_ui():
    return FileResponse("backend/status_ui.html")

@app.get("/signal_ui", response_class=HTMLResponse)
def signal_ui():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Å–∏–≥–Ω–∞–ª–æ–º.
    """
    return FileResponse("backend/signal_ui.html")

@app.get("/status")
def status():
    # –ü—Ä–∏–º–µ—Ä –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    model_loaded = os.path.exists("backend/signal_model.pth")
    vosk_model = os.path.exists(config.get("vosk_model_path", "backend/vosk-model-ru"))
    radioml_dataset = os.path.exists(config.get("radioml_dataset_path", "backend/radioml2018/RML2018.01A.h5"))
    errors = ""
    return {
        "model_loaded": model_loaded,
        "vosk_model": vosk_model,
        "radioml_dataset": radioml_dataset,
        "errors": errors
    }

@app.get("/", response_class=HTMLResponse)
def root():
    return """
    <!DOCTYPE html>
    <html lang="ru">
    <head>
        <meta charset="UTF-8">
        <title>Radio Decoder Starter</title>
        <style>
            body { font-family: Arial, sans-serif; background: #f0f4f8; }
            .container { background: #fff; max-width: 520px; margin: 40px auto; padding: 36px 28px 28px 28px; border-radius: 16px; box-shadow: 0 4px 24px rgba(0,0,0,0.10);}
            h1 { text-align: center; color: #1976d2; margin-bottom: 24px; }
            ul { list-style: none; padding: 0; }
            li { margin: 18px 0; }
            a, button { display: block; width: 100%; text-align: center; background: #1976d2; color: #fff; text-decoration: none; padding: 14px 0; border-radius: 8px; font-size: 1.1em; margin-bottom: 10px; border: none; cursor: pointer; transition: background 0.2s;}
            a:hover, button:hover { background: #125ea8; }
            .desc { text-align: center; color: #555; margin-bottom: 24px; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>Radio Decoder Starter</h1>
            <div class="desc">–ú–Ω–æ–≥–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞–¥–∏–æ—Å–∏–≥–Ω–∞–ª–æ–≤ –∏ —Ä–∞–±–æ—Ç—ã —Å –ò–ò</div>
            <ul>
                <li><a href="/settings_ui">‚öôÔ∏è –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏</a></li>
                <li><a href="/docs">üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è API (Swagger)</a></li>
                <li><a href="/signal_ui">üéõ –†–∞–±–æ—Ç–∞ —Å —Å–∏–≥–Ω–∞–ª–æ–º</a></li>
                <li><a href="/upload_ui">‚¨ÜÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤</a></li>
                <li><a href="/logs_ui">üìù –ü—Ä–æ—Å–º–æ—Ç—Ä –ª–æ–≥–æ–≤</a></li>
                <li><a href="/status_ui">üìä –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–µ—Ä–≤–µ—Ä–∞</a></li>
            </ul>
        </div>
    </body>
    </html>
    """

if config.get("use_anomaly_detector", False):
    # –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å config["anomaly_threshold"] –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ
    pass

if config.get("use_speech_recognition", False):
    # –≤–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
    pass

@app.post("/train_model")
def train_model():
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –≤ —Ñ–æ–Ω–µ (–º–æ–∂–Ω–æ –¥–æ—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ–¥ –≤–∞—à—É –û–°)
    subprocess.Popen(["python3", "backend/train_signal_model.py"])
    return {"status": "training started"}

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π (–æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ)
import torch
from backend.train_signal_model import ConvSignalNet, SAMPLE_LEN

@app.get("/signal_stats")
def signal_stats():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–ª–∞—Å—Å–∞–º —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.
    """
    return get_class_stats()

# –ó–∞–≥—Ä—É–∑–∫–∞ PyTorch-–º–æ–¥–µ–ª–∏
try:
    model_data = torch.load("backend/signal_model.pth", map_location="cpu")
    model_pytorch = ConvSignalNet(num_classes=len(model_data["classes"]))
    model_pytorch.load_state_dict(model_data["model_state"])
    model_pytorch.eval()
    model_pytorch.classes = model_data["classes"]
except Exception:
    model_pytorch = None

# –ó–∞–≥—Ä—É–∑–∫–∞ RF –∏ SVM –º–æ–¥–µ–ª–µ–π
try:
    model_rf = joblib.load("backend/rf_model.pkl")
except Exception:
    model_rf = None

try:
    model_svm = joblib.load("backend/svm_model.pkl")
except Exception:
    model_svm = None

sample_len = 1024  # –∏–ª–∏ SAMPLE_LEN, –µ—Å–ª–∏ –æ–Ω –æ–ø—Ä–µ–¥–µ–ª—ë–Ω –≤ train_signal_model.py

# TODO: –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–∞—Å—Ç–æ—Ç—É —Ñ–∞–π–ª–∞, —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å —Å –±–∞–∑–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤
# –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –æ —Ç–∏–ø–µ —Å–∏–≥–Ω–∞–ª–∞.

def find_band_by_freq(freq):
    for band in FREQ_DB:
        if band["start"] <= freq <= band["end"]:
            return band
    return None
